library(tidyverse)
library(readxl)
library(lubridate)
library(openxlsx)
library(zoo)



# Setup -------------------------------------------------------------------

# clean out exisiting environment
# helps to avoid overwriting
rm(list = ls())

# Choose submitted file to generate stats on 
filepath <- file.choose()


# read results tab of submitted file
Results_import <- read_excel(filepath, sheet = "Results")
colnames(Results_import) <- make.names(names(Results_import), unique=TRUE)


# convert F to C, filter out rejected data, and create datetime column
results_data <- Results_import %>%
  mutate(r = ifelse(Result.Unit == "deg F", round((Result.Value - 32)*(5/9),2), Result.Value),
         r_units = ifelse(Result.Unit == "deg F", "deg C", Result.Unit )) %>%
  filter(Result.Status.ID != "Rejected") %>%
  mutate(time_char = strftime(Activity.Start.Time, format = "%H:%M:%S", tz = 'UTC'),
         datetime = ymd_hms(paste(Activity.Start.Date, time_char)))


# get unique list of characteristics to run for loop through
unique_characteritics <- unique(Results_import$Characteristic.Name)

#create list for getting data out of loop
monloc_do_list <- list()
sumstatlist <- list()

# For loop for summary statistics -----------------------------------------


# Loop goes through each characteristc and generates summary stats
# After loop, data gets pushed inot single table
for (i in 1:length(unique_characteritics)){
  
  # Characteristic for this loop iteration
  char <- unique_characteritics[i]
  
  # Filter so table only contains single characteristic
  results_data_char <- results_data %>%
    filter(Characteristic.Name == char) %>%
    # generare unique hour field for hourly values and stats
    mutate(hr =  format(datetime, "%Y-%j-%H"))
  
  # Simplify to hourly values and Stats
  hrsum <- results_data_char %>%
    group_by(Monitoring.Location.ID, Equipment.ID.., hr, r_units, Activity.Start.End.Time.Zone) %>%
    summarise(date = mean(Activity.Start.Date),
              hrDTmin = min(datetime),
              hrDTmax = max(datetime),
              hrN = sum(!is.na(r)),
              hrMean = mean(r, na.rm=TRUE),
              hrMin = min(r, na.rm=TRUE),
              hrMax = max(r, na.rm=TRUE))
 
  
   # For each date, how many hours have hrN > 0
  # remove rows with zero records in an hour. 
  hrdat<- hrsum[which(hrsum$hrN >0),]
  
  # Summarise to daily statistics
  daydat <- hrdat %>%
    group_by(Monitoring.Location.ID, Equipment.ID.., date, r_units, Activity.Start.End.Time.Zone) %>%
    summarise(  dDTmin = min(hrDTmin),
                dDTmax = max(hrDTmax),
                hrNday = length(hrN), 
                dyN = sum(hrN),
                dyMean = mean(hrMean, na.rm=TRUE),
                dyMin = min(hrMin, na.rm=TRUE),
                dyMax = max(hrMax, na.rm=TRUE))
  
  daydat <- daydat %>%
    rowwise() %>%
    mutate(ResultStatusID = ifelse(hrNday >= 22, 'Final', "Rejected")) %>%
    mutate(cmnt =ifelse(hrNday >= 22, "Generated by ORDEQ", ifelse(hrNday <= 22 & hrNday >= 20, 
                                                                  paste0("Generated by ORDEQ; Estimated - ", as.character(hrNday), ' hrs with valid data in day' ), 
                                                                  paste0("Generated by ORDEQ; Rejected - ", as.character(hrNday), ' hrs with valid data in day' )) )) %>%
    mutate(ma.mean7 = as.numeric(""),
           ma.min7 = as.numeric(""),
           ma.mean30 = as.numeric(""),
           ma.max7 = as.numeric(""))
  
  
  #Deal with DO Results
  if (results_data_char$Characteristic.Name[1]  %in% c('DO','adjDO','DOs', "Dissolved oxygen (DO)")) {
    
    #monitoring location loop
    for(j in 1:length(unique(daydat$Monitoring.Location.ID))){
      
      station <- unique(daydat$Monitoring.Location.ID)[j]
      
      #Filter dataset to only look at 1 monitoring location at a time
      daydat_station <- daydat %>%
        filter(Monitoring.Location.ID == station) %>%
        mutate(startdate7 = as.Date(date) - 6,
               startdate30 = as.Date(date) -30)
      
     # 7 day loop
      # Loops throough each row in the monitoring location dataset
      # And pulls out records that are within the preceding 7 day window
      # If there are at least 6 values, then calculate 7 day min and mean
      # Assigns data back to daydat_station
       for(k in 1:nrow(daydat_station)){
        
         start7 <- daydat_station$startdate7[k]
         end7 <- daydat_station$date[k] 
         
         station_7day <- daydat_station %>%
           filter(date <= end7 & date >= start7) %>%
           filter(hrNday >= 22) 
         
         ma.mean7 <- ifelse(length(unique(station_7day$date)) >= 6, mean(station_7day$dyMean), NA )
         ma.min7 <- ifelse(length(unique(station_7day$date)) >= 6, min(station_7day$dyMean), NA )
         
         daydat_station[k,"ma.mean7"] <- ma.mean7
         daydat_station[k, "ma.min7"] <- ma.min7
         
         
        
        
      } #end of 7day loop
      
    # 30 day loop
      # Loops throough each row in the monitoring location dataset
      # And pulls out records that are within the preceding 30 day window
      # If there are at least 29 values, then calculate 30 day mean
      # Assigns data back to daydat_station
      for(l in 1:nrow(daydat_station)){
        
        
        start30 <- daydat_station$startdate30[l]
        end30 <- daydat_station$date[l] 
        
        station_30day <- daydat_station %>%
          filter(date <= end30 & date >= start30) %>%
          filter(hrNday >= 22) 
        
        ma.mean30 <- ifelse(length(unique(station_30day$date)) >= 29, mean(station_30day$dyMean), NA )
   
        
        daydat_station[l,"ma.mean30"] <- ma.mean30
   
      } #end of 30day loop
      
    # Assign dataset filtered to 1 monitoring location to a list for combining outside of for loop
      monloc_do_list[[j]] <- daydat_station
      
    } # end of monitoring location for loop
  
    # Combine list to single dataframe
    sum_stats <- bind_rows(monloc_do_list)    
    
    } # end of DO if statement
  
  
  ##  TEMPERATURE
  
  if (results_data_char$Characteristic.Name[1] %in% c('TEMP','adjTEMP', 'Temperature, water' )) {
    
    # Temperature is much easier to calculate, since it needs a complete 7 day record to calculate the 7day moving average
    # This can happen with a simple grouping
    sum_stats <- daydat %>%
      arrange(Monitoring.Location.ID, date) %>%
      group_by(Monitoring.Location.ID) %>%
      mutate(startdate7 = lag(date, 6, order_by = date),
             # flag out which result gets a moving average calculated
             calc7ma = ifelse(startdate7 == (as.Date(date) - 6), 1, 0 ))%>%
      mutate(ma.max7 = ifelse(calc7ma == 1, round(rollmean(x = dyMax, 7, align = "right", fill = NA),2) , NA )) %>%
      select(-startdate7, -calc7ma )
    
  } #end of temp if statement
 
  
  #Assign the char ID to the dataset
  sum_stats <- sum_stats %>%
    mutate(charID = char) 
  
  #Set to list for getting out of for loop
  sumstatlist[[i]] <-  sum_stats
  
  } # end of characteristics for loop



# Bind list to dataframe
sumstat <- bind_rows(sumstatlist)

#Gather summary statistics from wide format into long format
#rename summary statistcs to match AWQMS Import COnfiguration
sumstat_long <- sumstat %>%
  rename("Daily Maximum" = dyMax,
         "Daily Minimum" = dyMin,
         "Daily Mean"    = dyMean,
         "7DMADMin"      = ma.min7,
         "7DMADMean"     = ma.mean7,
         "7DMADMax"      = ma.max7,
         "30DMADMean"    = ma.mean30) %>%
  gather(
    "Daily Maximum",
    "Daily Minimum",
    "Daily Mean",
    "7DMADMin",
    "7DMADMean",
    "7DMADMax",
    "30DMADMean",
    key = "StatisticalBasis",
    value = "Result",
    na.rm = TRUE
  ) %>% 
  arrange(Monitoring.Location.ID, date) %>%
  rename(Equipment = Equipment.ID..)


# Read Audit Data ---------------------------------------------------------

Audit_import <- read_excel(filepath, sheet = "Audit_Data")
colnames(Audit_import) <- make.names(names(Audit_import), unique=TRUE)

# get rid of extra blankfields
Audits <- Audit_import %>%
  filter(!is.na(Project.ID))

# table of methods unique to project, location, equipment, char, and method
Audits_unique <- unique(Audits[c("Project.ID", "Monitoring.Location.ID", "Equipment.ID..", "Characteristic.Name", "Result.Analytical.Method.ID")])



# Reformat Audit info
# matches Dan Brown's import configuration
# If template has Result.Qualifier as column, use that value, if not use blank. 
Audit_info <- Audits %>%
  mutate(Result.Qualifier = ifelse("Result.Qualifier" %in% colnames(Audits), Result.Qualifier, "" ),
         Activity.Start.Time = as.character(strftime(Activity.Start.Time, format = "%H:%M:%S", tz = "UTC")),
         Activity.End.Time = as.character(strftime(Activity.End.Time, format = "%H:%M:%S", tz = "UTC")) ) %>%
  select(Project.ID, Monitoring.Location.ID, Activity.Start.Date,
         Activity.Start.Time, Activity.End.Date, Activity.End.Time,
         Activity.Start.End.Time.Zone, Activity.Type, 
         Activity.ID..Column.Locked., Equipment.ID.., Sample.Collection.Method,
         Characteristic.Name, Result.Value, Result.Unit, Result.Analytical.Method.ID,
         Result.Analytical.Method.Context, Result.Value.Type, Result.Status.ID,
         Result.Qualifier, Result.Comment)

#Write excel file for AWQMS import
write.xlsx(Audit_info, file = paste0(tools::file_path_sans_ext(filepath),"-Audits.xlsx") )



# AWQMS summary stats -----------------------------------------------------


# Join method to sumstat table
sumstat_long <- sumstat_long %>%
  left_join(Audits_unique, by = c("Monitoring.Location.ID", "charID" = "Characteristic.Name", "Equipment" = "Equipment.ID..") )

AQWMS_sum_stat <- sumstat_long %>%
  mutate(RsltTimeBasis = ifelse(StatisticalBasis == "7DMADMin" |
                                  StatisticalBasis == "7DMADMean" |
                                  StatisticalBasis == "7DMADMax", "7 Day", 
                                ifelse(StatisticalBasis == "30DMADMean", "30 Day", "1 Day" )),
         ActivityType = "FMC",
         SmplColMthd = "ContinuousPrb",
         SmplColEquip = "Probe/Sensor",
         SmplDepth = "",
         SmplDepthUnit = "",
         SmplColEquipComment = "",
         Samplers = "",
         Project = Project.ID,
         AnaStartDate = "",
         AnaStartTime = "",
         AnaEndDate = "",
         AnaEndTime = "",
         ActStartDate = format(dDTmax, "%Y-%m-%d"), 
         ActStartTime = format(dDTmax, "%H:%M:%S"),
         ActEndDate = format(dDTmax, "%Y-%m-%d"),
         ActEndTime = format(dDTmax, "%H:%M:%S"),
         RsltType = "Calculated",
         ActStartTimeZone = Activity.Start.End.Time.Zone,
         ActEndTimeZone = Activity.Start.End.Time.Zone,
         AnaStartTimeZone = "",
         AnaEndTimeZone = ""
    ) %>%
  select(charID,
         Result,
         r_units,
         Result.Analytical.Method.ID,
         RsltType,
         ResultStatusID,
         StatisticalBasis,
         RsltTimeBasis,
         cmnt,
         ActivityType,
         Monitoring.Location.ID,
         SmplColMthd,
         SmplColEquip,
         SmplDepth,
         SmplDepthUnit,
         SmplColEquipComment,
         Samplers,
         Equipment,
         Project,
         ActStartDate,
         ActStartTime,
         ActStartTimeZone,
         ActEndDate,
         ActEndTime,
         ActEndTimeZone,
         AnaStartDate,
         AnaStartTime,
         AnaStartTimeZone,
         AnaEndDate,
         AnaEndTime,
         AnaEndTimeZone)


# Export to same place as the originial file
write_csv(AQWMS_sum_stat, paste0(tools::file_path_sans_ext(filepath),"-statsum.csv"))



# Graphing ----------------------------------------------------------------


graph <- ggplot(results_data,aes(x = as.factor(Monitoring.Location.ID), y = r) )+
  geom_boxplot(fill = "gray83") +
  geom_jitter(width = 0.2, alpha = 0.1, color = "steelblue4") +
  facet_grid(Characteristic.Name ~ ., scales = 'free') +
  theme_bw() +
  xlab("Monitoring Location") +
  ylab("Result") + 
  theme(strip.background = element_blank())


ggsave(paste0(tools::file_path_sans_ext(filepath),"-Graph.png"), plot = graph)



# Deployment info ---------------------------------------------------------

deployments <- Results_import %>%
  mutate(time_char = strftime(Activity.Start.Time, format = "%H:%M:%S", tz = 'UTC'),
         datetime = ymd_hms(paste(Activity.Start.Date, time_char))) %>%
  group_by(Monitoring.Location.ID, Equipment.ID.., ) %>%
  summarise(startdate = min(datetime),
            enddate = max(datetime),
            TZ = first(Activity.Start.End.Time.Zone))

write.csv(deployments, paste0(tools::file_path_sans_ext(filepath),"-deployments.csv"), row.names = FALSE)











