library(tidyverse)
library(readxl)
library(lubridate)



#This is a current work in progess
#6/13/2018

#Do summary statistics need to be cleaned up


# this code takes the submitted 3rd party call for data continuous datasets and generates
# Summary statistics for loading into AWQMS. Output is a file to load into AWQMS
# Much of the code was taken from Steve Hanson's volunteer data processing scripts


# To run, source the file and select the submitted file to be processed



# Setup -------------------------------------------------------------------

# clean out exisiting environment
# helps to avoid overwriting
rm(list = ls())

# Choose submitted file to generate stats on 
filepath <- file.choose()


# read results tab of submitted file
Results_import <- read_excel(filepath, sheet = "Results")
colnames(Results_import) <- make.names(names(Results_import), unique=TRUE)


# convert F to C, filter out rejected data, and create datetime column
results_data <- Results_import %>%
  mutate(r = ifelse(Result.Unit == "deg F", round((Result.Value - 32)*(5/9),2), Result.Value),
         r_units = ifelse(Result.Unit == "deg F", "deg C", Result.Unit )) %>%
  filter(Result.Status.ID != "Rejected") %>%
  mutate(time_char = strftime(Activity.Start.Time, format = "%H:%M:%S", tz = 'UTC'),
         datetime = ymd_hms(paste(Activity.Start.Date, time_char)))


# get unique list of characteristics to run for loop through
unique_characteritics <- unique(Results_import$Characteristic.Name)

#create list for getting data out of loop
sumstatlist <- list()



# For loop for summary statistics -----------------------------------------


# Loop goes through each characteristc and generates summary stats
# After loop, data gets pushed inot single table
for (i in 1:length(unique_characteritics)){
  
  # Characteristic for this loop iteration
  char <- unique_characteritics[i]
  
  # Filter so table only contains single characteristic
  results_data_char <- results_data %>%
    filter(Characteristic.Name == char) %>%
    # generare unique hour field for hourly values and stats
    mutate(hr =  format(datetime, "%Y-%j-%H"))
  
  # Simplify to hourly values and Stats
  hrsumna <- results_data_char %>%
    group_by(Monitoring.Location.ID, Equipment.ID.., hr, r_units, Activity.Start.End.Time.Zone) %>%
    summarise(date = mean(Activity.Start.Date),
              hrDTmin = min(datetime),
              hrDTmax = max(datetime),
              hrN = sum(!is.na(r)),
              hrMean = mean(r, na.rm=TRUE),
              hrMin = min(r, na.rm=TRUE),
              hrMax = max(r, na.rm=TRUE))
  
  # Warnings are OK but need to run NA assignments below
  hrsumna$hrMin[which(is.infinite(hrsumna$hrMin))] <- NA
  hrsumna$hrMax[which(is.infinite(hrsumna$hrMax))] <- NA
  
  
  #########################
  # #
  #  #
  #   # 
  #  #  
  ##  aily stats
  ##########################
  
  # For each date, how many hours have hrN > 0
  # remove rows with zero records in an hour.
  hrdat<- hrsumna[which(hrsumna$hrN >0),]

  # Summarise to daily statistics
   daydat <- hrdat %>%
    group_by(Monitoring.Location.ID, Equipment.ID.., date, r_units, Activity.Start.End.Time.Zone) %>%
    summarise(  dDTmin = min(hrDTmin),
                dDTmax = max(hrDTmax),
                hrNday = length(hrN), 
                dyN = sum(hrN),
                dyMean = mean(hrMean, na.rm=TRUE),
                dyMin = min(hrMin, na.rm=TRUE),
                dyMax = max(hrMax, na.rm=TRUE)
               )
  

   # set cmts for periods with less than 24 hours of data
   daydat <- daydat %>%
     rowwise() %>%
     mutate(ResultStatusID = ifelse(hrNday > 20, 'Final', "Rejected")) %>%
     mutate(cmnt =ifelse(hrNday > 22, "Generated by ORDEQ", ifelse(hrNday <= 22 & hrNday > 20, 
                                                                   paste0("Generated by ORDEQ; Estimated - ", as.character(hrNday), ' hrs with valid data in day' ), 
                                                                   paste0("Generated by ORDEQ; Rejected - ", as.character(hrNday), ' hrs with valid data in day' )) ))  
                         
    
  #get daily median values
   dm <- results_data_char %>%
     ungroup() %>%
     group_by(Monitoring.Location.ID, Equipment.ID.., Activity.Start.Date) %>%
     summarise(dyMedian = median(r))
   
   daydat <- daydat %>%
     left_join(dm, by = c("Monitoring.Location.ID", "date" = "Activity.Start.Date"))
   
   ##########
   #   #
   ## ##
   # # #
   #   # oving Averages
   #########
   
   
   # create column for moving average calculations
   daydat$ma <- NA
   daydat$anaStart <- as.POSIXct(NA)# Add analysis start and end dates
   daydat$anaEnd <- as.POSIXct(NA)
  
    ##  DISSOLVED OXYGEN 
   ############ This is where we would generate stats for DO sat
   if (results_data_char$Characteristic.Name[1]  %in% c('DO','adjDO','DOs', "Dissolved oxygen (DO)")) {
     # remove data with bad dDQL's and get daily minimum value
     daydat$r4ma <- ifelse(daydat$ResultStatusID == 'Rejected' | is.na(daydat$ResultStatusID), NA, daydat$dyMin ) 
     for (j in 1:length(daydat$date)) {
       if (j < 30) {
         daydat$ma[j]<- as.numeric(NA)
       } else if (j >29 && (daydat$dDTmax[j] - daydat$dDTmin[j-29])<= 30.03) { # needs to be after the 30th day and should only span 30 days, the .03 accounts for time changes in fall .
         daydat$anaStart[j] <- as.character(daydat$dDTmin[j-29]) # careful that the local time zone doesn't mess this up
         daydat$anaEnd[j] <- as.character(daydat$dDTmax[j]) # careful that the timeshift doesn't mess this up
         ifelse(sum(is.na(daydat$r4ma[(j-29):j])) > 3, NA, # if more than 3 missing days than no calculation
                daydat$ma[j] <- mean(daydat$r4ma[(j-29):j], na.rm = TRUE))
       }
     }
   }
   
   ##  TEMPERATURE
   if (results_data_char$Characteristic.Name[1] %in% c('TEMP','adjTEMP', 'Temperature, water' )) {
     # remove data with bad dDQL's and get daily minimum value
     daydat$r4ma <- ifelse(daydat$ResultStatusID == 'Rejected' | is.na(daydat$ResultStatusID), NA, daydat$dyMax ) 
     for (j in 1:length(daydat$date)) {
       if (j < 7) {
         daydat$ma[j]<- as.numeric(NA)
       } else if (j > 6 && (daydat$dDTmax[j] - daydat$dDTmin[j-6]) <= 7.03) { # needs to be after the 6th day and should only span 7 days, the .03 accounts for time changes in fall .
         daydat$anaStart[j] <- as.character(daydat$dDTmin[j-6]) # careful that the default time zone doesn't mess this up
         daydat$anaEnd[j] <- as.character(daydat$dDTmax[j]) # careful that the the default time zone doesn't mess this up
         ifelse(sum(is.na(daydat$r4ma[(j-6):j])) > 1, NA, # if more than on missing day then no calculation
                daydat$ma[j] <- mean(daydat$r4ma[(j-6):j], na.rm = TRUE))
       }
     }
   }
   
   
   # Add deployment metadata
   daydat <- daydat %>%
     mutate(charID = char) 
   
   sumstatlist[[i]] <- daydat   
 
   
   
    }
  


# Finalizing --------------------------------------------------------------


sumstat <- bind_rows(sumstatlist)


#gather wide file to long format
sumstat_long <- sumstat %>%
  gather('dyMean', 'dyMin', 'dyMax', 'dyMedian', 'ma',  key = "StatisticalBasis", value = "Result") %>%
  arrange(Monitoring.Location.ID, date) %>%
  filter(!is.na(Result)) %>%
  select(-Equipment.ID...y) %>%
  rename(Equipment = Equipment.ID...x)
  

#read audit data
Audit_import <- read_excel(filepath, sheet = "Audit_Data")
colnames(Audit_import) <- make.names(names(Audit_import), unique=TRUE)

# get rid of ectra blankfields
Audits <- Audit_import %>%
  filter(!is.na(Project.ID))

# table of methods unique to project, location, equipment, char, and method
Audits_unique <- unique(Audits[c("Project.ID", "Monitoring.Location.ID", "Equipment.ID..", "Characteristic.Name", "Result.Analytical.Method.ID")])

# Join method to sumstat table
sumstat_long <- sumstat_long %>%
  left_join(Audits_unique, by = c("Monitoring.Location.ID", "charID" = "Characteristic.Name", "Equipment" = "Equipment.ID..") )


# Create fields needed tfor AWQMS and format to match Steve's AWQMS mport sheets
AQWMS_sum_stat <- sumstat_long %>%
  mutate(RsltTimeBasis = ifelse(StatisticalBasis == "ma", "7 Day", "1 Day" ),
         ActivityType = "FMC",
         SmplColMthd = "ContinuousPrb",
         SmplColEquip = "Probe/Sensor",
         SmplDepth = "",
         SmplDepthUnit = "",
         SmplColEquipComment = "",
         Samplers = "",
         Project = Project.ID,
         AnaStartDate = ifelse(StatisticalBasis == "ma", format(anaStart, "%Y-%m-%d"), format(date, "%Y-%m-%d")),
         AnaStartTime = ifelse(StatisticalBasis == "ma", format(anaStart, "%H:%M:%S"), format(date, "%H:%M:%S")),
         AnaEndDate = ifelse(StatisticalBasis == "ma", format(anaEnd, "%Y-%m-%d"),format(dDTmax, "%Y-%m-%d")),
         AnaEndTime = ifelse(StatisticalBasis == "ma", format(anaEnd, "%H:%M:%S"),  format(dDTmax, "%H:%M:%S")),
         ActStartDate = format(dDTmin, "%Y-%m-%d"),
         ActStartTime = format(dDTmin, "%H:%M:%S"),
         ActEndDate = format(dDTmax, "%Y-%m-%d"),
         ActEndTime = format(dDTmax, "%H:%M:%S"),
         RsltType = "Calculated",
         ActStartTimeZone = Activity.Start.End.Time.Zone,
         ActEndTimeZone = Activity.Start.End.Time.Zone,
         AnaStartTimeZone = Activity.Start.End.Time.Zone,
         AnaEndTimeZone = Activity.Start.End.Time.Zone,
         StatisticalBasis = ifelse(StatisticalBasis == "ma", "7DMADMax", StatisticalBasis )
         ) %>%
  select(charID,
         Result,
         r_units,
         Result.Analytical.Method.ID,
         RsltType,
         ResultStatusID,
         StatisticalBasis,
         RsltTimeBasis,
         cmnt,
         ActivityType,
         Monitoring.Location.ID,
         SmplColMthd,
         SmplColEquip,
         SmplDepth,
         SmplDepthUnit,
         SmplColEquipComment,
         Samplers,
         Equipment,
         Project,
         ActStartDate,
         ActStartTime,
         ActStartTimeZone,
         ActEndDate,
         ActEndTime,
         ActEndTimeZone,
         AnaStartDate,
         AnaStartTime,
         AnaStartTimeZone,
         AnaEndDate,
         AnaEndTime,
         AnaEndTimeZone)



# Export to same place as the originial file
write_csv(AQWMS_sum_stat, paste0(tools::file_path_sans_ext(filepath),"-statsum.csv"))



# Graphing ----------------------------------------------------------------


graph <- ggplot(results_data,aes(x = as.factor(Monitoring.Location.ID), y = r) )+
  geom_boxplot(fill = "gray83") +
  geom_jitter(width = 0.2, alpha = 0.1, color = "steelblue4") +
  facet_grid(Characteristic.Name ~ ., scales = 'free') +
  theme_bw() +
  xlab("Monitoring Location") +
  ylab("Result") + 
  theme(strip.background = element_blank())
  

ggsave(paste0(tools::file_path_sans_ext(filepath),"-Graph.png"), plot = graph)



# Deployment info ---------------------------------------------------------

deployments <- Results_import %>%
  mutate(time_char = strftime(Activity.Start.Time, format = "%H:%M:%S", tz = 'UTC'),
         datetime = ymd_hms(paste(Activity.Start.Date, time_char))) %>%
  group_by(Monitoring.Location.ID, Equipment.ID.., ) %>%
  summarise(startdate = min(datetime),
            enddate = max(datetime),
            TZ = first(Activity.Start.End.Time.Zone))

write_csv(deployments, paste0(tools::file_path_sans_ext(filepath),"-deployments.csv"))



# To Do -------------------------------------------------------------------

# Figure out what DO summary stats we need
# Deal with DOsat
# What about non-Do and temp data?

